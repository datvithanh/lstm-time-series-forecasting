{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class DataLoader():\n",
    "    '''Class for loading and transforming data for LSTM model'''\n",
    "    def __init__(self, filename_x, filename_y):\n",
    "        dataframe_x = pd.read_csv(filename_x, sep='\\s+',header=None)\n",
    "        dataframe_y = pd.read_csv(filename_y, sep='\\s+',header=None)\n",
    "        self.x = dataframe_x.values[:]\n",
    "        self.y = dataframe_y.values[:]\n",
    "        self.len = len(self.x)\n",
    "\n",
    "    def get_data(self):\n",
    "#         x = []\n",
    "#         y = []\n",
    "#         train_dim = math.floor(self.len/self.seq_len)\n",
    "#         for i in range(train_dim):\n",
    "#             start = i*self.seq_len\n",
    "#             end = (i+1)*self.seq_len\n",
    "#             x.append(self.x[start:end])\n",
    "#             y.append(self.y[start:end])\n",
    "            # print(i*self.seq_len, (i+1)*self.seq_len)\n",
    "        return np.array(self.x), np.array(self.y)\n",
    "\n",
    "    def _next_window(self, i, seq_len, normalise):\n",
    "        '''Generates the next data window from the given index location i'''\n",
    "        window = self.x[i:i+seq_len]\n",
    "        window = self.normalise_windows(window, single_window=True)[0] if normalise else window\n",
    "        x = window[:-1]\n",
    "        y = window[-1, [0]]\n",
    "        return x, y\n",
    "\n",
    "    def normalise_windows(self, window_data, single_window=False):\n",
    "        '''Normalise window with a base value of zero'''\n",
    "        normalised_data = []\n",
    "        window_data = [window_data] if single_window else window_data\n",
    "        for window in window_data:\n",
    "            normalised_window = []\n",
    "            for col_i in range(window.shape[1]):\n",
    "                normalised_col = [((float(p) / float(window[0, col_i])) - 1) for p in window[:, col_i]]\n",
    "                normalised_window.append(normalised_col)\n",
    "            normalised_window = np.array(normalised_window).T # reshape and transpose array back into original multidimensional format\n",
    "            normalised_data.append(normalised_window)\n",
    "        return np.array(normalised_data)\n",
    "    \n",
    "    def generate_train_batch(self, seq_len, batch_size, normalise):\n",
    "        '''Yield a generator of training data from filename on given list of cols split for train/test'''\n",
    "        i = 0\n",
    "        while i < (self.len - seq_len):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            for b in range(batch_size):\n",
    "                if i >= (self.len - seq_len):\n",
    "                    # stop-condition for a smaller final batch if data doesn't divide evenly\n",
    "                    yield np.array(x_batch), np.array(y_batch)\n",
    "                    i = 0\n",
    "                x, y = self._next_window(i, seq_len, normalise)\n",
    "                x_batch.append(x)\n",
    "                y_batch.append(y)\n",
    "                i += 1\n",
    "            yield np.array(x_batch), np.array(y_batch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] data: C1, optimizer: rmsprop\n",
      "Epoch: 1,  loss: 0.0009792015840539086\n",
      "Epoch: 2,  loss: 0.0008708160870880387\n",
      "Epoch: 3,  loss: 0.0007971533259256608\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "%matplotlib inline  \n",
    "\n",
    " \n",
    "# date-time parsing function for loading the dataset\n",
    "def parser(x):\n",
    "\treturn datetime.strptime('190'+x, '%Y-%m')\n",
    " \n",
    "# frame a sequence as a supervised learning problem\n",
    "def timeseries_to_supervised(data, lag=1):\n",
    "\tdf = DataFrame(data)\n",
    "\tcolumns = [df.shift(i) for i in range(1, lag+1)]\n",
    "\tcolumns.append(df)\n",
    "\tdf = concat(columns, axis=1)\n",
    "\tdf.fillna(0, inplace=True)\n",
    "\treturn df\n",
    " \n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "\tdiff = list()\n",
    "\tfor i in range(interval, len(dataset)):\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "\treturn Series(diff)\n",
    " \n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "\treturn yhat + history[-interval]\n",
    " \n",
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "\t# fit scaler\n",
    "\tscaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\tscaler = scaler.fit(train)\n",
    "\t# transform train\n",
    "\ttrain = train.reshape(train.shape[0], train.shape[1])\n",
    "\ttrain_scaled = scaler.transform(train)\n",
    "\t# transform test\n",
    "\ttest = test.reshape(test.shape[0], test.shape[1])\n",
    "\ttest_scaled = scaler.transform(test)\n",
    "\treturn scaler, train_scaled, test_scaled\n",
    " \n",
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value):\n",
    "\tnew_row = [x for x in X] + [value]\n",
    "\tarray = np.array(new_row)\n",
    "\tarray = array.reshape(1, len(array))\n",
    "\tinverted = scaler.inverse_transform(array)\n",
    "\treturn inverted[0, -1]\n",
    " \n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, batch_size, nb_epoch, neurons, optimizer):\n",
    "    X, y = train[:, 0:-1], train[:, -1] \n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    history = []\n",
    "    for i in range(nb_epoch):\n",
    "        hist = model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "        print(\"Epoch: {}, \".format(i+1),\"loss: {}\".format(hist.history['loss'][0]))\n",
    "        history.append(hist.history['loss'][0])\n",
    "        model.reset_states()\n",
    "    return model, history\n",
    " \n",
    "# make a one-step forecast\n",
    "def forecast_lstm(model, batch_size, X):\n",
    "\tX = X.reshape(1, 1, len(X))\n",
    "\tyhat = model.predict(X, batch_size=batch_size)\n",
    "\treturn yhat[0,0]\n",
    "\n",
    "project_path=\"./\"\n",
    "save_dir = project_path + \"saved_models/\"\n",
    "data_path = project_path + \"C1-6/C1-6_CanTho/\"\n",
    "filenames = [\"C1\"]\n",
    "# filenames = [\"C1\", \"C2\"]\n",
    "# optimizers = ['rmsprop', 'adagrad', 'adadelta', 'adam']\n",
    "optimizers = ['rmsprop']\n",
    "\n",
    "i = 0\n",
    "for filename in filenames:\n",
    "    training_data = DataLoader(data_path + filename + \"/Training_Input.txt\", data_path + filename + \"/Training_Target.txt\")\n",
    "    test_data = DataLoader(data_path + filename + \"/Testing_Input.txt\", data_path + filename + \"/Testing_Target.txt\")\n",
    "\n",
    "    x_train, y_train = training_data.get_data()\n",
    "    x_test, y_test = test_data.get_data()\n",
    "\n",
    "    train = np.concatenate((x_train, y_train), axis=1)\n",
    "    test = np.concatenate((x_test, y_test), axis=1)\n",
    "    for optimizer in optimizers:\n",
    "        print(\"[Model] data: {}, optimizer: {}\".format(filename, optimizer))\n",
    "        scaler, train_scaled, test_scaled = scale(train, test)\n",
    "        lstm_model, history = fit_lstm(train_scaled, 1, 3, 4, optimizer)\n",
    "        \n",
    "        lstm_model.save(save_dir +  '%s-%s.h5' % (filename, optimizer))\n",
    "        with open(save_dir + '%s-%s-history.json' % (filename, optimizer), 'w') as f:\n",
    "            json.dump(history, f)\n",
    "#         train_reshaped = train_scaled[:, 0:2].reshape(len(train_scaled), 1, 2)\n",
    "#         predict = lstm_model.predict(train_reshaped, batch_size=1)\n",
    "#         pyplot.plot(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
